# RAG Solution Troubleshooting Guide

## ‚úÖ **SOLUTION FIXED!**

The proxy errors have been resolved. The issue was with:
1. **Missing dependencies** - Fixed by installing correct package versions
2. **Backend not starting** - Fixed compatibility issues with sentence-transformers
3. **Model configuration** - Updated to use available models

## üöÄ **Quick Start (Working Solution)**

```bash
cd /Users/evinhua/genai/proj1/amazon_q
./start_rag.sh
```

This will:
- ‚úÖ Check if Ollama is running
- ‚úÖ Clean up any existing processes
- ‚úÖ Start backend on port 8000
- ‚úÖ Wait for backend to be ready
- ‚úÖ Start frontend on port 3000
- ‚úÖ Display access URLs

## üîç **Common Issues & Solutions**

### 1. **Proxy Errors (ECONNREFUSED)**
**Symptoms**: `Could not proxy request from localhost:3000 to http://localhost:8000`

**Solutions**:
- ‚úÖ **FIXED**: Backend now starts properly
- Use the new `start_rag.sh` script
- Backend health check: `curl http://localhost:8000/health`

### 2. **Backend Import Errors**
**Symptoms**: `ImportError: cannot import name 'cached_download'`

**Solutions**:
- ‚úÖ **FIXED**: Updated sentence-transformers to version 2.7.0
- Removed incompatible langchain dependencies
- All imports now work correctly

### 3. **Ollama Model Issues**
**Symptoms**: Models not found or embedding errors

**Solutions**:
- ‚úÖ **VERIFIED**: Both models are available
- `bge-m3:latest` - for embeddings
- `gemma3:4b` - for text generation (updated from gemma2:2b)
- Check with: `ollama list`

### 4. **Port Conflicts**
**Symptoms**: Address already in use

**Solutions**:
- ‚úÖ **HANDLED**: Script automatically cleans up ports
- Manual cleanup: `lsof -ti:8000 | xargs kill -9`
- Manual cleanup: `lsof -ti:3000 | xargs kill -9`

## üß™ **Testing the Solution**

### Backend Health Check
```bash
curl http://localhost:8000/health
# Expected: {"status":"healthy","ollama_status":"healthy",...}
```

### Frontend Access
```bash
curl http://localhost:3000/
# Expected: HTML content with React app
```

### API Documentation
Visit: http://localhost:8000/docs

## üìä **System Status**

### ‚úÖ **Working Components**
- FastAPI backend with Pydantic models
- React frontend with modern UI
- ChromaDB vector database
- Ollama integration (BGE-M3 + Gemma2:2b)
- Document processing (PDF, DOCX, PPTX)
- Multimodal image upload
- Real-time health monitoring

### üîß **Configuration**
- **Backend**: Python 3.11, FastAPI, ChromaDB
- **Frontend**: React 18, Modern CSS, Responsive design
- **Models**: BGE-M3 (embeddings), Gemma3:4b (generation)
- **Database**: ChromaDB with cosine similarity
- **CORS**: Configured for localhost:3000

## üö® **If Issues Persist**

1. **Check Ollama Service**:
   ```bash
   ollama serve  # Start Ollama if not running
   ollama list   # Verify models are available
   ```

2. **Verify Dependencies**:
   ```bash
   cd rag_backend
   source venv/bin/activate
   python -c "import sentence_transformers, chromadb, ollama; print('All imports OK')"
   ```

3. **Check Logs**:
   - Backend logs: Check terminal output
   - Frontend logs: Check browser console
   - Ollama logs: Check Ollama service output

4. **Reset Everything**:
   ```bash
   # Kill all processes
   pkill -f "python main.py"
   pkill -f "npm start"
   
   # Restart
   ./start_rag.sh
   ```

## üì± **Access URLs**

- **Frontend**: http://localhost:3000
- **Backend API**: http://localhost:8000
- **API Documentation**: http://localhost:8000/docs
- **Health Check**: http://localhost:8000/health

## üéâ **Success Indicators**

When everything is working, you should see:
- ‚úÖ Backend health check returns "healthy" status
- ‚úÖ Frontend loads without proxy errors
- ‚úÖ Document upload panel on the left
- ‚úÖ Chat interface on the right
- ‚úÖ System status bar shows all green
- ‚úÖ Can upload documents and ask questions

The solution is now fully functional!
